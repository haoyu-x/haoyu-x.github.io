<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haoyu Xiong</title>
  
  <meta name="author" content="Haoyu Xiong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="icon.png">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haoyu Xiong</name>
              </p>
              <p>Hi! My name is <a href="https://translate.google.com/?sl=en&tl=zh-CN&text=Haoyu%20Xiong&op=translate">Haoyu Xiong</a>. I am a graduate student researcher (MSR) studying AI and robotics in the <a href="https://www.ri.cmu.edu/ ">Robotics Institute</a> of the <a href="https://www.scs.cmu.edu/ ">School of Computer Science</a> at <a href="https://www.cs.cmu.edu/">Carnegie Mellon University</a>. 
		   I am fortunate to be advised by prof. <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a>ï¼Ž
		    

		    <!--  
		      and in collaboration with <a href="http://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a>, I also spent a wonderful time at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>.                                                                                                                                        
                     
		    I worked at <a href="https://iiis.tsinghua.edu.cn/en/yao/">Shanghai Qizhi Institute</a> a full-time researcher.
		      
		      advised by prof. <a href="https://scholar.google.com/citations?user=QZVQEWAAAAAJ&hl=en">Cewu Lu</a> and prof. <a href="http://hxu.rocks/">Huazhe Xu</a>. 
	     	 -->
		    </p> Follow me on <a href="https://twitter.com/Haoyu_Xiong_">Twitter</a> !

              <p>
              <p style="text-align:center">
                <a href="mailto:haoyux@andrew.cmu.edu">Email</a> &nbsp/&nbsp
		<a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=iGfCCEYAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/haoyu-x">Github</a> &nbsp/&nbsp
                <a href="">Linkedin</a> &nbsp/&nbsp

		      
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="haoyux-profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="haoyux-profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
               <p>
	         [Mar. 2024] Open-world Mobile Manipulation was highlighted in "Images of the Month" of <a href="https://www.nature.com/immersive/d41586-024-00608-5/index.html">Nautre</a> alongside other scientific phenomena such as NASAâ€™s space shuttle, solar flare, neon nerve cells.     
	       <br>
	        [Feb. 2024] Open-world Mobile Manipulation was featured by <a href="https://www.nature.com/articles/d41586-024-00488-9">Nautre AI & robotics briefing</a> and <a href="https://www.newscientist.com/article/2415590-this-robot-can-figure-out-how-to-open-almost-any-door-on-its-own/">New Scientist</a> .     
	       <br>
	        [Dec. 2023] Invited Talk @ Stanford Interactive Perception and Robot Learning Lab on "In Pursuit of Open-World Robotic Agents". 

	       <br>
                [Dec. 2022] Invited Talk @ Stanford Vision and Learning lab on RoboTube.
	       <br>
		[Sep. 2022] Our paper RoboTube has been accepted for an <strong>oral</strong> presentation for CoRL 2022!
	       <br>
		[Aug. 2022] Joined CMU RI as a graduate student.
	       <br>
                [Apr. 2022] Invited Talk @ Tsinghua University on Robot Learning from Videos.
	       </p>
            </td>
          
		
	      
	      
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>

          </tr>
		

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='spin.gif' width="170" height="90"></div>
                <img src='spin.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://open-world-mobilemanip.github.io/" >
                <papertitle>SPIN: Simultaneous Perception, Interaction and Navigation</papertitle>
              </a>
              <br>
              Shagun Uppal, Ananye Agarwal,<strong>Haoyu Xiong</strong>, Kenneth Shaw, Deepak Pathak   

        
             <br>
	      IEEE/CVF Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>), 2024
      <br>
		Sim2real mobile manipulation with a 2-DOF active camera. All with a single end-to-end neural network.
		    <p></p>
              <a href="https://spin-robot.github.io/">website</a>     

            </td>
          </tr>

		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='door_gif.gif' width="170" height="90"></div>
                <img src='door_gif.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://open-world-mobilemanip.github.io/" >
                <papertitle>Adaptive Mobile Manipulation for Articulated Objects In the Open World </papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>, Russell Mendonca, Kenny Shaw, Deepak Pathak.
              <br>
	      Under Review
              <br>
	       Robots in the open-ended real world such as homes has been a long-standing challenge.  However, in current research, robots are often studied in tabletop lab settings.
	       We provide a full-stack approach for open-world mobile manipulation of everyday tasks. 
              <p></p>
              <a href="https://open-world-mobilemanip.github.io/">website</a>     

            </td>
          </tr>
		
        
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='robotube_tesaser.gif' width="160" height="90"></div>
                <img src='robotube_tesaser.gif' width="160" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">
                <papertitle>RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments</papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>,  <a href="">Haoyuan Fu</a>, <a href="">Jieyi Zhang</a>,  <a href="">Chen Bao</a>,  <a href="">Qiang Zhang</a>, <a href="">Yongxi Huang</a>, <a href="">Wenqiang Xu</a>, 
              <a href="https://animesh.garg.tech/">Animesh Garg</a>, <a href="http://hxu.rocks/">Huazhe Xu</a>, <a href="https://scholar.google.com/citations?user=QZVQEWAAAAAJ&hl=en">Cewu Lu</a>
              <br>
	      Dataset, baselines, and simulated twin environments for robot learning from human videos. 
              <br>
	      RoboTube aims to lower the barrier to robot learning from videos research for beginners while facilitating reproducible research in the community.
              <br>             
	      ðŸŽ‰ <strong>Oral(6.5%)</strong> Conference on Robot Learning (<strong>CoRL</strong>), 2022
	      <br>
              <a href="https://www.robotube.org/">robotube.org</a> /    
	      <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">paper</a> 
              <p></p>
            </td>
          </tr>
                
        
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='goal.gif' width="170"></div>
                <img src='goal.gif' width="170">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9636080" >
                <papertitle>Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>, <a href="https://quanzhou-li.github.io/">Quanzhou Li</a>, <a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>, <a href="https://homangab.github.io/">Homanga Bharadhwaj</a>, <a href="https://www.samsinha.me/">Samrath Sinha</a>, 
              <br>
              <a href="https://animesh.garg.tech/">Animesh Garg</a>
              <br>
	      IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2021
              <br>             
              ðŸŽ‰ Spotlight Talk, <strong>RSS 2021</strong> <a href="https://rssvlrr.github.io/">Workshop on Visual Learning and Reasoning for Robotics</a> 
              <br>
              <a href="https://www.pair.toronto.edu/lbw-kp/">website</a> / 
	      <a href="https://www.youtube.com/watch?v=Retu1q-BbEo&feature=emb_logo">video</a> / 
	      <a href="https://arxiv.org/pdf/2101.07241.pdf">pdf</a> / 
	      <a href="https://www.youtube.com/watch?v=4rTORPjokBk&list=PL7f1xXvV_1Ms3pRZsCBbBvpP2RWDCUe4z&index=6">live talk</a>
              <p></p>
            </td>
          </tr>
		
		

		              
		

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Service</heading>
              <p>
               Reviewer: ICRA 22', 23', RA-L 23', ICLR 24', CVPR 24'
              </p>
            </td>
          </tr>

		
        

	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
