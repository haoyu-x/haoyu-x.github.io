<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haoyu Xiong</title>
  
  <meta name="author" content="Haoyu Xiong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!-- 	<link rel="icon" href="icon.png">
 -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haoyu Xiong</name>
              </p>
              <p>I am a student researcher at <a href="https://src.stanford.edu/">Stanford University</a> advised by Prof. <a href="https://shurans.github.io/">Shuran Song</a>. I earned my M.S. in <a href="https://www.ri.cmu.edu/ ">Robotics</a> at <a href="https://www.cs.cmu.edu/">Carnegie Mellon University</a> advised by Prof. <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a>. 
	              </p>
		  <p>     Please email me at haoyux.me@gmail.com, as my CMU email has expired.</a>.



	    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/Haoyu_Xiong_" class="twitter-follow-button" data-show-count="false">Follow @Haoyu_Xiong_</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

              <p>
              <p style="text-align:center">
                <a href="mailto:haoyux.me@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=iGfCCEYAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/haoyu-x">Github</a> &nbsp/&nbsp
                <a href="">Linkedin</a> &nbsp/&nbsp
		      

		      
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="haoyux-profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="haoyux-profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
		    
               <p>
	         [Mar. 2024] Our mobile manipulation research was highlighted in "Images of the Month" of <a href="https://www.nature.com/immersive/d41586-024-00608-5/index.html" style="color:rgb(255, 100, 100); font-weight: bold;">Nature</a> alongside other scientific phenomena such as NASAâ€™s space shuttle, solar flare, neon nerve cells.     
	       <br>
	        [Feb. 2024] Open-world Mobile Manipulation was featured by <a href="https://www.nature.com/articles/d41586-024-00488-9">Nautre AI & robotics briefing</a> and <a href="https://www.newscientist.com/article/2415590-this-robot-can-figure-out-how-to-open-almost-any-door-on-its-own/">New Scientist</a> .     

	       <br>
                [Dec. 2022] Invited Talk @ <a href="https://svl.stanford.edu/">Stanford Vision and Learning lab </a> on Robot Learning from Human Videos.
	       <br>
		[Aug. 2022] Joined CMU RI as a graduate student.
	       <br>
                [Apr. 2022] Invited Talk @ Tsinghua University on Robot Learning from Videos.
	       </p>
            </td>
          
<!DOCTYPE HTML>
<html lang="en">

<head>
    <title>Research</title>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" type="images/png" href="images/favicon.png">
    <link rel="canonical" href="https://yourwebsite.com/">
    <link href='https://fonts.googleapis.com/css?family=Nunito' rel='stylesheet'>

    <style>
        img {
            vertical-align: middle;
        }

        .research-row {
            display: flex;
            align-items: center;
            margin-bottom: 30px;
        }

        .image-container {
            width: 25%;
            text-align: center;
        }

        .text-container {
            width: 75%;
            padding-left: 20px;
        }

        h3 {
            margin: 0;
            padding-bottom: 10px;
        }
    </style>

    <script>
        function npil_start() {
            document.getElementById('npil_image').style.opacity = "1";
        }

        function npil_stop() {
            document.getElementById('npil_image').style.opacity = "0";
        }
        npil_stop()
    </script>
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr style="padding:0px">
            <td style="padding:0px">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" border="0">
                    <tr style="padding:0px">
		            <td style="padding:20px;width:100%;vertical-align:middle">
				    
				<heading>Research</heading>
                        </td>
                    </tr>
                </table>
<!-- 
                <div class="research-row" onmouseout="npil_stop()" onmouseover="npil_start()">
                    <div class="image-container">
                        <img src='roboneck.gif' width="170" height="90">
                    </div>
                    <div class="text-container">
                        <a href="https://robot-neck.github.io/">
                            <h4>RoboNeck: Visuomotor Policy Learning with Active Perception</h4>
                        </a>
                        <strong>Haoyu Xiong</strong>, Xiaomeng Xu, Jimmy Wu, Yifan Hou, Jeannette Bohg, Shuran Song.
                        <br>
			"See What Robot Sees".
			<br><br>
                        <a href="https://robot-neck.github.io/">website</a>
                    </div>
                </div>
 -->
		    
                <div class="research-row" onmouseout="npil_stop()" onmouseover="npil_start()">
                    <div class="image-container">
                        <img src='door_gif.gif' width="170" height="90">
                    </div>
                    <div class="text-container">
                        <a href="https://open-world-mobilemanip.github.io/">
                            <h4>Adaptive Mobile Manipulation for Articulated Objects in the Open World</h4>
                        </a>
                        <strong>Haoyu Xiong</strong>, Russell Mendonca, Kenny Shaw, Deepak Pathak.
                        <br>
			A full-stack approach for mobile manipulation robots learning to <strong>open doors</strong> in the real world.
			<br><br>
                        <a href="https://www.nature.com/immersive/d41586-024-00608-5/index.html" style="color:rgb(255, 100, 100); font-weight: bold;">Nature report</a> / <a href="https://open-world-mobilemanip.github.io/">website</a>
                    </div>
                </div>

                <div class="research-row" onmouseout="npil_stop()" onmouseover="npil_start()">
                    <div class="image-container">
                        <img src='bidex.gif' width="170" height="90">
                    </div>
                    <div class="text-container">
                        <a href="https://bidex-teleop.github.io/">
                            <h4>Bimanual Dexterity for Complex Tasks</h4>
                        </a>
                        Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar Srirama, Ray Liu, <strong>Haoyu Xiong</strong>, Russell Mendonca, Deepak Pathak.
                        <br>
	                CoRL 2024
			<br><br>
                        <a href="https://bidex-teleop.github.io/">website</a> / <a href="https://openreview.net/forum?id=55tYfHvanf">paper</a>
                    </div>
                </div>

                <div class="research-row" onmouseout="npil_stop()" onmouseover="npil_start()">
                    <div class="image-container">
                        <img src='spin.gif' width="170" height="90">
                    </div>
                    <div class="text-container">
                        <a href="https://spin-robot.github.io/">
                            <h4>SPIN: Simultaneous Perception, Interaction, and Navigation</h4>
                        </a>
                        Shagun Uppal, Ananye Agarwal, <strong>Haoyu Xiong</strong>, Kenneth Shaw, Deepak Pathak.
                        <br>
			CVPR 2024
                        <br>
                        <br><br>
                        <a href="https://spin-robot.github.io/">website</a>
                    </div>
                </div>

		    
                <div class="research-row" onmouseout="npil_stop()" onmouseover="npil_start()">
                    <div class="image-container">
                        <img src='robotube_tesaser.gif' width="170" height="90">
                    </div>
                    <div class="text-container">
                        <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">
                            <h4>RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments</h4>
                        </a>
                        <strong>Haoyu Xiong</strong>, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, Huazhe Xu, Cewu Lu.
                        <br>
                
			    <strong><span style="color:rgb(255, 100, 100); font-weight: bold;">Oral(6.5%)</span></strong> CoRL 2022 
			                  <br>  <br>
                        <a href="https://www.robotube.org/">robotube.org</a> / <a href="https://youtu.be/nTz94xUrCtg?feature=shared&t=1671">oral talk video</a>
                    </div>
                </div>

		    
                <div class="research-row" onmouseout="npil_stop()" onmouseover="npil_start()">
                    <div class="image-container">
                        <img src='goal.gif' width="170">
                    </div>
                    <div class="text-container">
                        <a href="https://ieeexplore.ieee.org/abstract/document/9636080">
                            <h4>Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</h4>
                        </a>
                        <strong>Haoyu Xiong</strong>, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samrath Sinha, Animesh Garg.
                        <br>
	
                        IROS 2021
                        <br>
			<strong><span style="color:rgb(255, 100, 100); font-weight: bold;">Spotlight Talk</span></strong> RSS 2021 <a href="https://rssvlrr.github.io/">workshop</a> 
              <br>  <br>
              <a href="https://www.pair.toronto.edu/lbw-kp/">website</a> / 
	      <a href="https://www.youtube.com/watch?v=Retu1q-BbEo&feature=emb_logo">video</a> / 
	      <a href="https://arxiv.org/pdf/2101.07241.pdf">pdf</a> / 
	      <a href="https://www.youtube.com/watch?v=4rTORPjokBk&list=PL7f1xXvV_1Ms3pRZsCBbBvpP2RWDCUe4z&index=6">spotlight talk video</a>
              <p></p>

			    

			                        </div>
                </div>
            </td>
        </tr>
    </table>
</body>



<!-- 	      
	      
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>

          </tr>
		

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='door_gif.gif' width="170" height="90"></div>
                <img src='door_gif.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://open-world-mobilemanip.github.io/" >
                <papertitle>Adaptive Mobile Manipulation for Articulated Objects In the Open World </papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>, Russell Mendonca, Kenny Shaw, Deepak Pathak.
	       Robots in the open-ended real world such as homes has been a long-standing challenge.  However, in current research, robots are often studied in tabletop lab settings.
	       We provide a full-stack approach for open-world mobile manipulation of everyday tasks. 
              <p></p>
	<a href="https://www.nature.com/immersive/d41586-024-00608-5/index.html" style="color:rgb(255, 100, 100); font-weight: bold;">Nature report</a> /
              <a href="https://open-world-mobilemanip.github.io/">website</a>     

            </td>
          </tr>
		

		
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='bidex.png' width="170" height="90"></div>
                <img src='bidex.png' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://bidex-teleop.github.io/" >
                <papertitle>Bimanual Dexterity for Complex Tasks</papertitle>
              </a>
	       <br>
		Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar Srirama, Ray Liu, <strong>Haoyu Xiong</strong>, Russell Mendonca, Deepak Pathak
		    	      

	      <br>
	      Conference on Robot Learning (<strong>CoRL</strong>), 2024
	      <br>
                Bimanual teleoperation systems for over 50DOF that accurately tracks both arms and dexterous hands in the real world. <p></p>
              <a href="https://bidex-teleop.github.io/">website</a> / 
              <a href="https://openreview.net/forum?id=55tYfHvanf">paper</a>     

            </td>
          </tr>


		
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='spin.gif' width="170" height="90"></div>
                <img src='spin.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://spin-robot.github.io/" >
                <papertitle>SPIN: Simultaneous Perception, Interaction and Navigation</papertitle>
              </a>
              <br>
              Shagun Uppal, Ananye Agarwal,<strong>Haoyu Xiong</strong>, Kenneth Shaw, Deepak Pathak   

        
             <br>
	      IEEE/CVF Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>), 2024
      <br>
		Sim2real mobile manipulation with a 2-DOF active camera. All with a single end-to-end neural network.
		    <p></p>
              <a href="https://spin-robot.github.io/">website</a>     

            </td>
          </tr>

	
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='robotube_tesaser.gif' width="160" height="90"></div>
                <img src='robotube_tesaser.gif' width="160" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">
                <papertitle>RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments</papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, Huazhe Xu, Cewu Lu		                  <br>
	      Dataset, baselines, and simulated twin environments for robot learning from human videos. 
              <br>
	      RoboTube aims to lower the barrier to robot learning from videos research for beginners while facilitating reproducible research in the community.
              <br>             
	      ðŸŽ‰ <strong>Oral(6.5%)</strong> Conference on Robot Learning (<strong>CoRL</strong>), 2022
	      <br>
              <a href="https://www.robotube.org/">robotube.org</a> /    
	      <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">paper</a> 
              <p></p>
            </td>
          </tr>
                
        
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='goal.gif' width="170"></div>
                <img src='goal.gif' width="170">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9636080" >
                <papertitle>Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</papertitle>
              </a>
              <br>
	        <strong>Haoyu Xiong</strong>, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samrath Sinha, Animesh Garg
              <a href="https://quanzhou-li.github.io/">Quanzhou Li</a>, <a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>, <a href="https://homangab.github.io/">Homanga Bharadhwaj</a>, <a href="https://www.samsinha.me/">Samrath Sinha</a>, 
              <br>
              <a href="https://animesh.garg.tech/">Animesh Garg</a>
              <br>
	      IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2021
              <br>             
              ðŸŽ‰ Spotlight Talk, <strong>RSS 2021</strong> <a href="https://rssvlrr.github.io/">Workshop on Visual Learning and Reasoning for Robotics</a> 
              <br>
              <a href="https://www.pair.toronto.edu/lbw-kp/">website</a> / 
	      <a href="https://www.youtube.com/watch?v=Retu1q-BbEo&feature=emb_logo">video</a> / 
	      <a href="https://arxiv.org/pdf/2101.07241.pdf">pdf</a> / 
	      <a href="https://www.youtube.com/watch?v=4rTORPjokBk&list=PL7f1xXvV_1Ms3pRZsCBbBvpP2RWDCUe4z&index=6">live talk</a>
              <p></p>
            </td>
          </tr>
		
		 -->

		              
	

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr style="padding:0px">
            <td style="padding:0px">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" border="0">
                    <tr style="padding:0px">
		            <td style="padding:20px;width:100%;vertical-align:middle">
	
        		      <heading>Professional Service</heading>
              <p>
               Reviewer: ICRA 22', 23', RA-L 23' 24', ICLR 24' 25', CVPR 24', CoRL 24', AAAI25. 
              </p>
            </td>
          </tr>

		 


	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
