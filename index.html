<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haoyu Xiong</title>
  
  <meta name="author" content="Haoyu Xiong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="icon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--  <link rel="shortcut icon" href="icon.ico"> -->

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haoyu Xiong</name>
              </p>
				
              <p>
				  I am a PhD student in the <a href="https://www.eecs.mit.edu/">Department of EECS</a> at <a href="https://web.mit.edu/">MIT</a>. I am fortunate to be co-advised by Prof. <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a> and Prof. <a href="https://yilundu.github.io/">Yilun Du</a>.
	         </p>  
				              <p>

 		 Previously, I was fortunate to have worked with Prof. <a href="https://shurans.github.io/">Shuran Song</a> at <a href="https://src.stanford.edu/">Stanford University</a>, and Prof. <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a> at <a href="https://www.cs.cmu.edu/">Carnegie Mellon University</a>. 
	              </p> 

								  				              <p>

				  <strong>Email</strong>: haoyux [at] csail.mit.edu

	         </p> 


			
	    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/Haoyu_Xiong_" class="twitter-follow-button" data-show-count="false">Follow @Haoyu_Xiong_</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		</p>
		
              <p style="text-align:center">
                <a href="mailto:haoyux@csail.mit.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=iGfCCEYAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/haoyu-x">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/haoyu-xiong-03b122174/">Linkedin</a> 
              </p>
		
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="haoyux-profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="haoyux-profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>



	  

       </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
     
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>


				
               <p>
				   	[Aug. 2025] Joined MIT EECS as a PhD student.

				   <br>
				   [Aug. 2025] <a href="https://vision-in-action.github.io/">Vision in Action</a> was accepted to CoRL 2025 â€” see you in Seoul!	    

	      			 <br>
				   	[June 2025] Co-organized the workshop on <a href="https://rss-moma-2025.github.io/">Mobile Manipulation: Emerging Opportunities & Contemporary Challenges</a>  at RSS 2025.
	                <br>
			        [May. 2025] Wrapped up a wonderful visit at <a href="https://real.stanford.edu/">REAL</a> at Stanford University. Thank you, Shuran!
	                  <br>		   
	                [Mar. 2024] Our mobile manipulation research was highlighted in "Images of the Month" of <a href="https://www.nature.com/immersive/d41586-024-00608-5/index.html" style="color:rgb(255, 100, 100); font-weight: bold;">Nature</a> alongside other scientific phenomena such as NASAâ€™s space shuttle, solar flare, neon nerve cells.     
	                  <br>
	                [Feb. 2024] Open-world Mobile Manipulation was featured by <a href="https://www.nature.com/articles/d41586-024-00488-9">Nautre AI & robotics briefing</a> and <a href="https://www.newscientist.com/article/2415590-this-robot-can-figure-out-how-to-open-almost-any-door-on-its-own/">New Scientist</a> .     
<!-- 
	               <br>
                [Dec. 2022] Invited Talk @ <a href="https://svl.stanford.edu/">Stanford Vision and Learning lab </a> on Robot Learning from Human Videos.
	       <br>
		[Aug. 2022] Joined CMU RI as a graduate student. -->

				   	       </p>
            </td>
          
<!DOCTYPE HTML>
<html lang="en">

       </tbody></table>



	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Highlights</heading>

<!-- 
		  
        <table style="width:100%;border:0px;border-spacing:20px;border-collapse:separate;margin-right:auto;margin-left:auto;" border="0">
          <tr>
            <td style="width:50%;vertical-align:top">
              <iframe width="100%" height=auto src="https://www.youtube.com/embed/QiRICWD1v5w" 
                title="YouTube video player" frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                allowfullscreen>
              </iframe>
              <p style="text-align:center;margin-top:10px;font-size:14px">
                <a href="https://vision-in-action.github.io/">Robot Active Perception System (A.K.A. Robot Neck)</a>
              </p>
            </td>

            <td style="width:50%;vertical-align:top">
              <iframe width="100%" height=auto src="https://www.youtube.com/embed/QITyEao86ro" 
                title="YouTube video player" frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                allowfullscreen>
              </iframe>
              <p style="text-align:center;margin-top:10px;font-size:14px">
                <a href="https://open-world-mobilemanip.github.io/">Mobile Manipulation Robot that Learns to Open Doors</a>
              </p>
            </td>
          </tr>
        </table>
 -->

<table style="width:100%;border:0px;border-spacing:20px;border-collapse:separate;margin:auto;" border="0">
  <tr>
    <td style="width:50%;vertical-align:top;text-align:center;">
      <iframe 
        src="https://www.youtube.com/embed/QiRICWD1v5w"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen
        style="width:80%;aspect-ratio:16/9;">
      </iframe>
      <p style="text-align:center;margin-top:10px;font-size:14px">
        <a href="https://vision-in-action.github.io/">Robot Active Perception System (A.K.A. Robot Neck)</a>
      </p>
    </td>

    <td style="width:50%;vertical-align:top;text-align:center;">
      <iframe 
        src="https://www.youtube.com/embed/QITyEao86ro"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen
        style="width:80%;aspect-ratio:16/9;">
      </iframe>
      <p style="text-align:center;margin-top:10px;font-size:14px">
        <a href="https://open-world-mobilemanip.github.io/">Mobile Manipulation Robot that Learns to Open Doors</a>
      </p>
    </td>
  </tr>
</table>

			


		  
      </td>
    </tr>
  </tbody>
</table>


	
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Projects</heading>
            </td>

          </tr>
		

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='via_teaser.gif' width="170" height="90"></div>
                <img src='via_teaser.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://vision-in-action.github.io/" >
                <papertitle>Vision in Action: Learning Active Perception from Human Demonstrations</papertitle>
              </a>
	       <br>
		<strong>Haoyu Xiong</strong>, Xiaomeng Xu, Jimmy Wu, Yifan Hou, Jeannette Bohg, Shuran Song

		    	      

	      <br>
	      <strong>CoRL</strong> 2025
	      <br>
<!--                 An active perception system (A.K.A <strong>a Robot Neck</strong>) for bimanual robot manipulation. -->
<!-- 				<p>					 -->
<!-- 				</p> -->
		    	      </p>

		    <a href="https://vision-in-action.github.io/">website</a> / 
              <a href="https://github.com/haoyu-x/vision-in-action">fully open-sourced</a>     

            </td>
          </tr>


	
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='door_gif.gif' width="170" height="90"></div>
                <img src='door_gif.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://open-world-mobilemanip.github.io/" >
                <papertitle>Adaptive Mobile Manipulation for Articulated Objects In the Open World </papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>, Russell Mendonca, Kenny Shaw, Deepak Pathak.
				
<!-- 	      <br> -->
<!-- 	      <strong>Tech Report</strong> 2024 -->
<!-- 	      <br> -->
				
<!-- 	       Robots in the open-ended real world such as homes has been a long-standing challenge.  However, in current research, robots are often studied in tabletop lab settings. -->
<!-- 	       We provide a full-stack approach for open-world mobile manipulation of everyday tasks.  -->
<!--               <p> -->
<!-- 	      </p> -->
<!--               <a href="https://open-world-mobilemanip.github.io/">website</a>      -->
						    	      </p>

 	<a href="https://www.nature.com/immersive/d41586-024-00608-5/index.html" style="color:rgb(255, 100, 100); font-weight: bold;">Nature report</a> /
              <a href="https://open-world-mobilemanip.github.io/">website</a>      

            </td>
          </tr>
		

		
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='bidex_teaser.gif' width="170" height="90"></div>
                <img src='bidex_teaser.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://bidex-teleop.github.io/" >
                <papertitle>Bimanual Dexterity for Complex Tasks</papertitle>
              </a>
	       <br>
		Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar Srirama, Ray Liu, <strong>Haoyu Xiong</strong>, Russell Mendonca, Deepak Pathak
		    	      

	      <br>
	      <strong>CoRL</strong> 2024
	      <br>
<!--                 Bimanual teleoperation systems for over 50DOF that accurately tracks both arms and dexterous hands in the real world. <p></p>  -->
<!--               	      </p> -->
		    	      </p>

		    <a href="https://bidex-teleop.github.io/">website</a> / 
              <a href="https://openreview.net/forum?id=55tYfHvanf">paper</a>     

            </td>
          </tr>


		
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='spin.gif' width="170" height="90"></div>
                <img src='spin.gif' width="170" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://spin-robot.github.io/" >
                <papertitle>SPIN: Simultaneous Perception, Interaction and Navigation</papertitle>
              </a>
              <br>
              Shagun Uppal, Ananye Agarwal, <strong>Haoyu Xiong</strong>, Kenneth Shaw, Deepak Pathak   

        
             <br>
	      <strong>CVPR</strong> 2024
      <br>
<!-- 		Sim2real mobile manipulation with a 2-DOF active camera. All with a single end-to-end neural network. -->
<!-- 		    <p></p>  -->
		    	      </p>

              <a href="https://spin-robot.github.io/">website</a>     

            </td>
          </tr>

	
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
		<img src='robotube_tesaser.gif' width="160" height="90"></div>
                <img src='robotube_tesaser.gif' width="160" height="90">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">
                <papertitle>RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments</papertitle>
              </a>
              <br>
              <strong>Haoyu Xiong</strong>, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, Huazhe Xu, Cewu Lu		                  
<!-- 				<br> -->
<!-- 	      Dataset for robot learning from human videos.  -->
<!--               <br> -->
<!-- 	      RoboTube aims to lower the barrier to robot learning from videos research for beginners while facilitating reproducible research in the community. -->
              <br>             
	      ðŸŽ‰ <strong>Oral(6.5%)</strong> <strong>CoRL</strong>, 2022
	      <br> 
		    	      </p>

              <a href="https://www.robotube.org/">website</a> /    
	      <a href="https://proceedings.mlr.press/v205/xiong23a/xiong23a.pdf">paper</a> 
              <p></p>
            </td>
          </tr>
                
        
		
           <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='goal.gif' width="170"></div>
                <img src='goal.gif' width="170">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:85%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9636080" >
                <papertitle>Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</papertitle>
              </a>
              <br>
	        <strong>Haoyu Xiong</strong>, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samrath Sinha, Animesh Garg
<!--               <a href="https://quanzhou-li.github.io/">Quanzhou Li</a>, <a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>, <a href="https://homangab.github.io/">Homanga Bharadhwaj</a>, <a href="https://www.samsinha.me/">Samrath Sinha</a>,  -->
<!--               <br> -->
<!--               <a href="https://animesh.garg.tech/">Animesh Garg</a> -->
               <br>
	         <strong>IROS</strong> 2021
              <br>             
              ðŸŽ‰ Spotlight Talk, <strong>RSS 2021</strong> <a href="https://rssvlrr.github.io/">Workshop on Visual Learning and Reasoning for Robotics</a> 
              <br> 
		    	      </p>

              <a href="https://www.pair.toronto.edu/lbw-kp/">website</a> / 
	      <a href="https://www.youtube.com/watch?v=Retu1q-BbEo&feature=emb_logo">video</a> / 
	      <a href="https://arxiv.org/pdf/2101.07241.pdf">pdf</a> / 
	      <a href="https://www.youtube.com/watch?v=4rTORPjokBk&list=PL7f1xXvV_1Ms3pRZsCBbBvpP2RWDCUe4z&index=6">live talk</a>
              <p></p>
            </td>
          </tr>
		
	

		              
<!-- 	

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr style="padding:0px">
            <td style="padding:0px">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" border="0">
                    <tr style="padding:0px">
		            <td style="padding:20px;width:100%;vertical-align:middle">
	
        		      <heading>Professional Service</heading>
              <p>
               Reviewer: ICRA 22', 23', RA-L 23' 24', ICLR 24' 25', CVPR 24', CoRL 24', AAAI25. 
              </p>
            </td>
          </tr>

		  -->


	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>	 
